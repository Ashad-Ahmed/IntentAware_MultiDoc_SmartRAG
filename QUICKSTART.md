# Quick Start Guide - Smart Multi-Doc RAG v2.0

Get your production-grade RAG system running in 5 minutes!

## âš¡ Quick Setup (Windows)

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```
â±ï¸ Takes 2-3 minutes (downloads ML models on first run)

### 2. Start the Server
```bash
python main.py
```

Expected output:
```
INFO:     Uvicorn running on http://0.0.0.0:8000
```

### 3. Try It Out!

#### Option A: Using the Web Interface
- Open browser â†’ `http://localhost:8000/docs`
- Interactive Swagger UI for all endpoints

#### Option B: Using Command Line

**Upload a document:**
```bash
curl -X POST "http://localhost:8000/upload" -F "file=@path/to/your/document.pdf"
```

**Query the documents:**
```bash
curl -X POST "http://localhost:8000/query?query_text=What%20is%20main%20topic"
```

**View documents:**
```bash
curl "http://localhost:8000/documents"
```

#### Option C: Using Python
```python
import requests

# Upload
files = {"file": open("guide.pdf", "rb")}
r = requests.post("http://localhost:8000/upload", files=files)
print(r.json())

# Query
r = requests.post(
    "http://localhost:8000/query",
    params={"query_text": "Your question here"}
)
print(r.json()["context"])
```

## ðŸ“ Supported File Formats

| Format | Status | Notes |
|--------|--------|-------|
| **PDF** | âœ… Supported | Text-based PDFs (not scanned) |
| **DOCX** | âœ… Supported | Word documents with tables |
| **TXT** | âœ… Supported | Plain text files |
| **CSV** | ðŸ”„ Coming | Phase 3 |
| **Markdown** | ðŸ”„ Coming | Phase 3 |

## ðŸŽ¯ Key Endpoints

### 1. Upload (`POST /upload`)
Upload any PDF, DOCX, or TXT file
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@myfile.pdf"
```

### 2. Query (`POST /query`)
Ask a question about your documents
```bash
curl -X POST "http://localhost:8000/query" \
  -d "query_text=How does it work?"
```

### 3. List Documents (`GET /documents`)
See all uploaded documents
```bash
curl "http://localhost:8000/documents"
```

### 4. Delete Document (`DELETE /documents/{id}`)
Remove a document by ID
```bash
curl -X DELETE "http://localhost:8000/documents/1"
```

### 5. Statistics (`GET /stats`)
View system metrics
```bash
curl "http://localhost:8000/stats"
```

## ðŸš€ What's Working Now (Phase 1 & 2)

âœ… PDF extraction and chunking
âœ… DOCX document parsing
âœ… Multi-document querying
âœ… Intent-aware retrieval
âœ… Hybrid search (semantic + keyword)
âœ… Cross-encoder reranking
âœ… SQLite persistence
âœ… REST API with full documentation
âœ… Query analytics and logging
âœ… Error handling and validation

## ðŸ“Š Example Query Flow

1. **Upload documents**
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@machine_learning_guide.pdf"
```

2. **Query the system**
```bash
curl -X POST "http://localhost:8000/query" \
  -d "query_text=Explain neural networks"
```

3. **Get intelligent context**
```json
{
  "status": "success",
  "query": "Explain neural networks",
  "intent": "definition",
  "context": "[machine_learning_guide]\nNeural networks are...",
  "retrieved_chunks": 6,
  "response_time_ms": 234.5
}
```

## ðŸ”§ Configuration

Most settings in `.env` work out of the box. Common tweaks:

**For better accuracy:**
- Keep defaults (CHUNK_WORDS=140)

**For faster responses on large collections:**
- Reduce `TOP_K_VECTOR` from 80 to 40
- Reduce `TOP_K_FINAL_CHUNKS` from 6 to 3

**For handling large files:**
- Increase `MAX_FILE_SIZE` in `.env`
- Default is 50MB

## ðŸ“ Project Layout

```
SmartMultiDocRAG/
â”œâ”€â”€ main.py              â† Start here!
â”œâ”€â”€ config.py            â† Configuration
â”œâ”€â”€ database.py          â† Data persistence
â”œâ”€â”€ document_processor.py â† PDF/DOCX extraction
â”œâ”€â”€ rag_core.py         â† Retrieval logic
â”œâ”€â”€ requirements.txt     â† Dependencies
â”œâ”€â”€ .env                â† Environment settings
â”œâ”€â”€ README.md           â† Full documentation
â””â”€â”€ data/               â† Auto-created
    â”œâ”€â”€ documents/      â† Uploaded files
    â”œâ”€â”€ vector_db/      â† Embeddings
    â””â”€â”€ rag_system.db  â† Database
```

## ðŸ› Troubleshooting

**Can't start server?**
```bash
# Make sure port 8000 is available
# Or change PORT in .env to 8080, etc.
```

**No documents found?**
```bash
# Check what's uploaded
curl "http://localhost:8000/documents"

# Upload a test file
curl -X POST "http://localhost:8000/upload" \
  -F "file=@test.txt"
```

**Models downloading slowly?**
- First run downloads models (~500MB)
- Models cached in `~/.cache/huggingface/`
- Allow 1-2 minutes on first startup

**Query returns empty context?**
- Ensure documents are processed: `GET /documents`
- Try simpler, more specific queries
- Check `retrieved_chunks > 0` in response

## ðŸ“š Next Steps

1. **Upload your documents** via `/docs` interface
2. **Try different queries** to test retrieval
3. **Check statistics** at `/stats`
4. **View API docs** at `/docs` for more details

## ðŸ“– Full Documentation

See `README.md` for:
- Detailed API documentation
- Architecture overview
- Configuration reference
- Advanced usage examples
- Performance tuning guide

## ðŸ¤” Common Questions

**Q: How many documents can it handle?**
A: Tested up to 100+ documents. Performance depends on document size and system RAM.

**Q: Can I use it in production?**
A: Yes! Phase 1 & 2 are production-ready. See Phase 3 for enterprise features (auth, monitoring).

**Q: How accurate is the retrieval?**
A: Varies by query quality. Intent-aware ranking helps. Rerank more documents for complex queries.

**Q: Can I use my own embedding model?**
A: Yes, edit `EMBEDDING_MODEL` in `.env`. Any HuggingFace sentence transformer works.

---

**Ready to go?** Start with:
```bash
python main.py
# Then visit http://localhost:8000/docs
```

Happy querying! ðŸš€
